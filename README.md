# Pneumonia Detection

### Problem
I am doing my [General Assembly Data Science Immersive](https://generalassemb.ly/education/data-science-immersive) Capstone project on image recognition. Specifically, I am building an algorithm to detect a visual signal for pneumonia in medical images with a reach goal of automatically locating lung opacities on chest radiographs.  To this end I am using a convolutional neural network in order to first classify each image.  

### Data
The data I have came from a [Kaggle Competition - RSNA Pneumonia Detection Challenge](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). In that dataset are two `.csv` files detailing patient information and each image's class along with approximately 29,000 DICOM files which include an x-ray image combined with other patient meta-data per file.

The metrics I am currently focusing on are accuracy and loss. I am also looking into optimizing for false negatives as accidentally telling a sick person that they're fine and can go home has a much higher potential for harm than telling someone who is fine that they need to get more tests.

### Findings
There are two options regarding the classification of each image. The first option is to classify into two categories: Pneumonia and Not Pneumonia. This limits the requirements of the model and accomplishes the given task. It also has generally allowed my models to score better overall than the second option for classifying with an average accuracy score around 80%. The second option is to classify into three separate categories: Pneumonia, Normal, and Not Pneumonia / Not Normal. This increases the potential for the model to misclassify images and my models have shown a decrease in performance by approximately 15%, averaging around 65% accurate so far.

### Assumptions, Limitations, and Potential Risks
There are a number of limitations that present a problem during this project. First and foremost is the issue regarding hardware. The images provided are 1024 pixels by 1024 pixels. Training a model on images with this high of a resolution would not only take an incredibly long time, I can't even load them all into memory. Ideally I would be able to rent out an EC2 instance through Amazon Web Services but they have yet to get back to me regarding using a more powerful instance and running this kind of instance over a long period of time would become costly very quickly. Throughout this project I've been able to produce models based off of images scaled down to 128px by 128px but I would like to try training my models on higher resolution images to see if they perform better. Another limitation that arises from hardware concerns is the amount of time it takes to train each model (it takes 3+ hours). This limits the amount of hyper-parameter tuning that can be efficiently done.

One major assumption that this project is based on is that all of these images are 100% accurately classified. I am not a radiologist nor do I have the time to go through all ~29k images but in my experience people aren't always on top of their game. I had x-rays taken for an ACL issue and the radiologist said that the images looked fine and nothing was wrong. My doctor had to call him up and ask him to look again because everything was not fine as, in fact, I did not have an ACL whatsoever. So with that said, I would find it certainly believable if there were a number of misclassified images in the data set which would then throw off my model's accuracy.  

There is also the risk that this dataset is inherently biased based on where these images were sourced from. There are many different factors that could have an effect on x-ray images and ~29k is likely not a big enough sample size to accurately predict against the whole of the population. This algorithm's effectiveness would likely depend on the similarity of the surrounding region to the region of origin for the images.
